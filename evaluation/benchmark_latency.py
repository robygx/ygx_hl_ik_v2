"""
PiM-IK éª¨å¹²ç½‘ç»œå»¶è¿Ÿä¸æ˜¾å­˜åŸºå‡†æµ‹è¯•

ç”¨äºæ¶ˆèå®éªŒï¼šå¯¹æ¯” Mamba / LSTM / Causal Transformer åœ¨å•å¸§æµå¼æ¨ç†åœºæ™¯ä¸‹çš„
- è®¡ç®—å»¶è¿Ÿ (Latency)
- å³°å€¼æ˜¾å­˜å ç”¨ (Peak VRAM)
- æ¨¡å‹å‚æ•°é‡ (Parameters)

ä½¿ç”¨åœºæ™¯ï¼šæœºå™¨äººå®æ—¶æ§åˆ¶ï¼Œè¦æ±‚ä½å»¶è¿Ÿ (<10ms for 1kHz control)

ä½œè€…: PiM-IK é¡¹ç›®
æ—¥æœŸ: 2025-02-28
"""

import torch
import time
from pim_ik_net import PiM_IK_Net


def format_number(num: float, unit: str = "") -> str:
    """æ ¼å¼åŒ–æ•°å­—ï¼Œä¿ç•™åˆé€‚çš„å°æ•°ä½æ•°"""
    if num >= 1000:
        return f"{num/1000:.1f}K{unit}"
    elif num >= 1:
        return f"{num:.2f}{unit}"
    else:
        return f"{num*1000:.1f}m{unit}"


def benchmark_model(model: torch.nn.Module, T_ee: torch.Tensor,
                    num_warmup: int = 100, num_iters: int = 1000) -> dict:
    """
    åŸºå‡†æµ‹è¯•å•ä¸ªæ¨¡å‹

    Args:
        model: å¾…æµ‹è¯•æ¨¡å‹
        T_ee: è¾“å…¥å¼ é‡
        num_warmup: é¢„çƒ­æ¬¡æ•°
        num_iters: æµ‹è¯•è¿­ä»£æ¬¡æ•°

    Returns:
        dict: åŒ…å« latency_ms, vram_mb, params ç­‰æŒ‡æ ‡
    """
    model.eval()
    device = T_ee.device

    # ç»Ÿè®¡å‚æ•°é‡
    num_params = sum(p.numel() for p in model.parameters())

    # é¢„çƒ­ï¼šè®© GPU é¢‘ç‡ç¨³å®š
    with torch.no_grad():
        for _ in range(num_warmup):
            _ = model(T_ee)
        torch.cuda.synchronize()

    # æ˜¾å­˜æµ‹è¯•
    torch.cuda.reset_peak_memory_stats()
    torch.cuda.synchronize()

    with torch.no_grad():
        _ = model(T_ee)
    torch.cuda.synchronize()

    peak_vram_mb = torch.cuda.max_memory_allocated() / 1024 / 1024

    # å»¶è¿Ÿæµ‹è¯•
    torch.cuda.synchronize()
    start_time = time.perf_counter()

    with torch.no_grad():
        for _ in range(num_iters):
            _ = model(T_ee)

    torch.cuda.synchronize()
    end_time = time.perf_counter()

    total_time_s = end_time - start_time
    avg_latency_ms = (total_time_s / num_iters) * 1000

    return {
        'latency_ms': avg_latency_ms,
        'vram_mb': peak_vram_mb,
        'params': num_params
    }


def print_markdown_table(results: dict):
    """æ‰“å°ç²¾ç¾çš„ Markdown è¡¨æ ¼"""
    print("\n" + "=" * 80)
    print("PiM-IK éª¨å¹²ç½‘ç»œæ€§èƒ½åŸºå‡†æµ‹è¯•ç»“æœ")
    print("=" * 80)
    print("\næµ‹è¯•é…ç½®:")
    print("  - è¾“å…¥å½¢çŠ¶: (B=1, W=30, 4, 4)")
    print("  - éšç©ºé—´ç»´åº¦: d_model=256")
    print("  - å †å å±‚æ•°: num_layers=4")
    print("  - é¢„çƒ­æ¬¡æ•°: 100")
    print("  - æµ‹è¯•è¿­ä»£: 1000")
    print("  - è®¾å¤‡: CUDA")

    print("\n" + "| Model | Params | Latency | Peak VRAM |")
    print("|-------|--------|---------|-----------|")

    for model_name, metrics in results.items():
        params_k = metrics['params'] / 1000
        latency = metrics['latency_ms']
        vram = metrics['vram_mb']

        # å‚æ•°é‡åˆ—
        params_str = f"{params_k:.0f}K"

        # å»¶è¿Ÿåˆ—ï¼ˆæ ¹æ®æ˜¯å¦é€‚åˆå®æ—¶æ§åˆ¶ç€è‰²ï¼‰
        if latency < 10:
            latency_str = f"**{latency:.2f}** âœ…"  # é€‚åˆ 1kHz æ§åˆ¶
        elif latency < 33:
            latency_str = f"{latency:.2f} âš ï¸"  # é€‚åˆ 30Hz æ§åˆ¶
        else:
            latency_str = f"{latency:.2f} âŒ"  # å»¶è¿Ÿè¿‡é«˜

        # æ˜¾å­˜åˆ—
        vram_str = f"{vram:.1f}"

        print(f"| {model_name} | {params_str} | {latency_str} | {vram_str} |")

    print("\nè¯´æ˜:")
    print("  - âœ… : å»¶è¿Ÿ < 10msï¼Œé€‚åˆ 1kHz å®æ—¶æ§åˆ¶")
    print("  - âš ï¸  : å»¶è¿Ÿ < 33msï¼Œé€‚åˆ 30Hz è§†è§‰ä¼ºæœ")
    print("  - âŒ : å»¶è¿Ÿè¿‡é«˜ï¼Œä¸é€‚åˆå®æ—¶æ§åˆ¶")
    print("=" * 80 + "\n")


def main():
    # æ£€æŸ¥ CUDA å¯ç”¨æ€§
    if not torch.cuda.is_available():
        print("é”™è¯¯: éœ€è¦ CUDA è®¾å¤‡è¿è¡ŒåŸºå‡†æµ‹è¯•")
        return

    device = torch.device('cuda:0')

    # æµ‹è¯•é…ç½®ï¼ˆä¸è®­ç»ƒè„šæœ¬ä¿æŒä¸€è‡´ï¼‰
    d_model = 256
    num_layers = 4
    batch_size = 1
    window_size = 30

    # æ„é€ è¾“å…¥å¼ é‡ï¼ˆæ¨¡æ‹ŸçœŸå®æ¨ç†ç¯å¢ƒï¼‰
    T_ee = torch.randn(batch_size, window_size, 4, 4, device=device)

    print(f"\nåˆå§‹åŒ–æ¨¡å‹...")
    print(f"  è®¾å¤‡: {device}")
    print(f"  è¾“å…¥å½¢çŠ¶: {T_ee.shape}")

    # åˆå§‹åŒ–ä¸‰ä¸ªæ¨¡å‹
    models = {
        'Mamba': PiM_IK_Net(d_model=d_model, num_layers=num_layers, backbone_type='mamba'),
        'LSTM': PiM_IK_Net(d_model=d_model, num_layers=num_layers, backbone_type='lstm'),
        'Transformer': PiM_IK_Net(d_model=d_model, num_layers=num_layers, backbone_type='transformer'),
    }

    # å°†æ¨¡å‹ç§»åˆ° GPU
    for name, model in models.items():
        models[name] = model.to(device)
        print(f"  âœ… {name} å·²åŠ è½½åˆ° GPU")

    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    results = {}
    for name, model in models.items():
        print(f"\næ­£åœ¨æµ‹è¯• {name}...")
        results[name] = benchmark_model(model, T_ee)
        print(f"  å»¶è¿Ÿ: {results[name]['latency_ms']:.3f} ms")
        print(f"  æ˜¾å­˜: {results[name]['vram_mb']:.1f} MB")
        print(f"  å‚æ•°: {results[name]['params']:,}")

    # æ‰“å°ç»“æœè¡¨æ ¼
    print_markdown_table(results)

    # é¢å¤–åˆ†æ
    print("ğŸ“Š æ€§èƒ½å¯¹æ¯”åˆ†æ:")

    # æ‰¾å‡ºæœ€å¿«çš„æ¨¡å‹
    fastest = min(results.items(), key=lambda x: x[1]['latency_ms'])
    print(f"  â€¢ æœ€å¿«: {fastest[0]} ({fastest[1]['latency_ms']:.2f} ms)")

    # è®¡ç®—ç›¸å¯¹ Mamba çš„åŠ é€Ÿæ¯”
    if 'Mamba' in results:
        mamba_latency = results['Mamba']['latency_ms']
        for name, metrics in results.items():
            if name != 'Mamba':
                ratio = metrics['latency_ms'] / mamba_latency
                if ratio > 1:
                    print(f"  â€¢ Mamba ç›¸æ¯” {name} å¿« {ratio:.2f}x")
                else:
                    print(f"  â€¢ {name} ç›¸æ¯” Mamba å¿« {1/ratio:.2f}x")

    # å®æ—¶æ§åˆ¶é€‚ç”¨æ€§
    print(f"\nğŸ¤– å®æ—¶æ§åˆ¶é€‚ç”¨æ€§ (1kHz æ§åˆ¶ < 10ms):")
    for name, metrics in results.items():
        status = "âœ… é€‚åˆ" if metrics['latency_ms'] < 10 else "âŒ ä¸é€‚åˆ"
        print(f"  â€¢ {name}: {metrics['latency_ms']:.2f} ms -> {status}")


if __name__ == "__main__":
    main()
